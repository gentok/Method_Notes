{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA208 Machine Learning Memo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 8 (04/27/2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin Based Methods\n",
    "\n",
    "#### Logistic Regression Loss\n",
    "\n",
    " * **Model** <br>\n",
    " DV: $y \\in \\{-1, 1\\}$ <br>\n",
    " Prediction: $\\hat{y} = 1\\{x^\\top \\hat{\\beta} + \\hat{\\beta}_0 \\geq 0\\}$\n",
    " \n",
    " * **Loss** <br>\n",
    "  * 0-1 loss (what we want to minimize, but nonconvex): \n",
    "  \n",
    "  $$\\mathit{l}_{0/1}(y_i, x_i \\beta) = 1\\{y_i \\cdot x_i^\\top \\beta \\leq 0 \\}$$ <br>\n",
    "  \n",
    "  * Logistic loss (actually minimized in logit): \n",
    "  \n",
    "  \\begin{align}\n",
    "  \\mathit{l}_{logit} (y_i, x_i, \\beta) &=\n",
    "  \\begin{cases}\n",
    "  log(1 + e^{x_i^\\top \\beta}), & y_i = -1 \\\\\n",
    "  log(1 + e^{-x_i^\\top \\beta}), & y_i = 1  \n",
    "  \\end{cases}\n",
    "  \\\\ &= log(1+e^{-y_i x_i^T \\beta})\n",
    "  \\end{align} <br>\n",
    "  \n",
    "  * Square error loss (we can also use but...):\n",
    "  \n",
    "  $$\\mathit{l}_2(y_i, x_i, \\beta) = (y_i - x_i^\\top \\beta)^2 = (1- y_i x_i^\\top \\beta)^2$$ <br>\n",
    "  \n",
    "  * Logistic loss approximates 0-1 loss more than square error loss, but **computationally inefficient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
